Our implementation involves using an external macros called uthash that allows us to create, insert, sort, and iterate through a hashtable. The main data structure is a a hash table of distinct words, each of which points to a sorted linked list of records. We first begin by recursively accessing a file or a directory with nested directories and/or files. On each file, we use tokenizer.c to tokenize the files and insert the distinct words into a hashtable (This is nearly the same tokenizer.c given in the assignment but adjusted accordingly). We insert all the distinct words from every text file into the hashtable. Once the hashtable has been built, we sort the keys, which are the words themselves. We then iterate through the hashtable and write all the words and their list of records (file name and frequency) into an output text file.  

The first step involves accessing all the files in an existing directory. The best case running time of accessing the files is O(1) and this occurs when there is only one file to be read with no directories. The worst case running time of accessing the files is O(n + k), where n is the number of directories and k is number of text filess. Big O(n+k) occurs when there are n directories and k files to be read. 

When reading a file and tokenizing the words, the best case and worst case running time is O(x), where x is the number of words in the file. 

The running time of inserting the words and their values into a hashtable is worst case big O(xg), where there are x words that are tokenized and g records to be traversed in the linked list that holds the file name and their frequency of occurrence.

The running time of writing to a file is worst case big O(m + g), where m is the number of distinct words tokenized and g is the number of records traversed altogether.

Much of the memory used in our program is used to store the keys and their values in the hashtable. The memory space that is dynamically allocated and later freed is for the hash table struct and each record struct. The overall running time of our program in the worst case is O(xg) since xg is more significant than m + g and n + k. Note that x is the number of words tokenized and g is the number of records altogether. In the best case, the running time of the program is O(x), when each word added is distinct and no sorting of records is required.
