We used an external macros called uthash to generate the data structure that stores the words and their filenames. Our overall implementation involves generating an index file from indexer.c and then using that index file in search.c to query different words. First, we parse the inverted-index file and place the words in a hashtable, as the keys. The values of each key is a linked list of the filenames and their frequencies. After generating the hashtable we have an infite while loop for constant querying. If the user enters sa or so, a function pertaining to one of those inputs is called.

The indexFiles function parses the inverted index file and generates a hash table data structure to hold the words and their file names. The worst case and best case running time will be the same. The running time will be big O(n*m*j) where n is the line numbers in the file and m are the tokens in a line and j are the tokens in the lines after <list>. The value of j is always the value of m after <list> because we are copying the tokens in that line into dynamic memory. Since, n and j are less than m, the overall big O for this function is O(m).

The main function does the constant querying. Once the user enters sa/so and some words, the while loop tokenizes the line and calls the appropriate function while tokenizing. The best case running time for this function is big O (n), where n is the number of words in the line. This occurs when the user only enters one entry. The worst case running time is big O(n * m), where m is the number of searches the user completes. 

The worst case and best case running time of SO is O(n-1) or big O(n) because the function is called for every token in the line after so. 

The overall running time of our program is big O(m) because parsing the index file and inserting it into the data structure is the most time consuming function. 
